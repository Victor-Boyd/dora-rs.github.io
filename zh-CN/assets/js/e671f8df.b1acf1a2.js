"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[4914],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>m});var o=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function p(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=o.createContext({}),s=function(e){var n=o.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=s(e.components);return o.createElement(i.Provider,{value:n},e.children)},d="mdxType",_={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},c=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,i=e.parentName,u=p(e,["components","mdxType","originalType","parentName"]),d=s(t),c=r,m=d["".concat(i,".").concat(c)]||d[c]||_[c]||a;return t?o.createElement(m,l(l({ref:n},u),{},{components:t})):o.createElement(m,l({ref:n},u))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,l=new Array(a);l[0]=c;var p={};for(var i in n)hasOwnProperty.call(n,i)&&(p[i]=n[i]);p.originalType=e,p[d]="string"==typeof e?e:r,l[1]=p;for(var s=2;s<a;s++)l[s]=t[s];return o.createElement.apply(null,l)}return o.createElement.apply(null,t)}c.displayName="MDXCreateElement"},4706:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>l,default:()=>_,frontMatter:()=>a,metadata:()=>p,toc:()=>s});var o=t(7462),r=(t(7294),t(3905));const a={},l="yolop \u64cd\u4f5c\u7b26",p={unversionedId:"nodes_operators/yolop_op",id:"nodes_operators/yolop_op",title:"yolop \u64cd\u4f5c\u7b26",description:"yolop \u4ece\u7279\u5b9a\u56fe\u50cf\u4e2d\u8bc6\u522b\u8f66\u9053\u548c\u53ef\u9a7e\u9a76\u533a\u57df\u3002",source:"@site/i18n/zh-CN/docusaurus-plugin-content-docs/current/nodes_operators/yolop_op.md",sourceDirName:"nodes_operators",slug:"/nodes_operators/yolop_op",permalink:"/zh-CN/docs/nodes_operators/yolop_op",draft:!1,editUrl:"https://crowdin.com/dora-rs/zh-CN",tags:[],version:"current",frontMatter:{},sidebar:"nodes_operators",previous:{title:"Webcam (\u6444\u50cf\u5934) \u64cd\u4f5c\u7b26",permalink:"/zh-CN/docs/nodes_operators/webcam_op"},next:{title:"Yolov5 \u64cd\u4f5c\u7b26",permalink:"/zh-CN/docs/nodes_operators/yolov5_op"}},i={},s=[{value:"\u8f93\u5165",id:"\u8f93\u5165",level:2},{value:"\u8f93\u51fa",id:"\u8f93\u51fa",level:2},{value:"\u793a\u4f8b\u7ed8\u5236 (\u8f66\u9053\u662f\u7ea2\u8272\u7684\uff0c\u53ef\u9a7e\u9a76\u533a\u57df\u662f\u7eff\u8272\u7684)",id:"\u793a\u4f8b\u7ed8\u5236-\u8f66\u9053\u662f\u7ea2\u8272\u7684\u53ef\u9a7e\u9a76\u533a\u57df\u662f\u7eff\u8272\u7684",level:2},{value:"\u56fe\u63cf\u8ff0",id:"\u56fe\u63cf\u8ff0",level:2},{value:"\u65b9\u6cd5",id:"\u65b9\u6cd5",level:2},{value:"<code>__init__()</code>",id:"__init__",level:3},{value:"<code>.on_event(...)</code>",id:"on_event",level:3},{value:"<code>.on_input(...)</code>",id:"on_input",level:3}],u={toc:s},d="wrapper";function _(e){let{components:n,...t}=e;return(0,r.kt)(d,(0,o.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"yolop-\u64cd\u4f5c\u7b26"},"yolop \u64cd\u4f5c\u7b26"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"yolop")," \u4ece\u7279\u5b9a\u56fe\u50cf\u4e2d\u8bc6\u522b\u8f66\u9053\u548c\u53ef\u9a7e\u9a76\u533a\u57df\u3002"),(0,r.kt)("p",null,"\u66f4\u591a\u4fe1\u606f\u5728\u6b64: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/hustvl/YOLOP"},"https://github.com/hustvl/YOLOP")),(0,r.kt)("p",null,"\u60a8\u8fd8\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u73af\u5883\u53d8\u91cf\u5728 GPU \u4e2d\u5206\u914d\u6a21\u578b\uff1a"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"PYTORCH_DEVICE: cuda # \u6216 cpu"))),(0,r.kt)("h2",{id:"\u8f93\u5165"},"\u8f93\u5165"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"\u56fe\u50cf: \u9ad8 x \u5bbd x BGR array.")),(0,r.kt)("h2",{id:"\u8f93\u51fa"},"\u8f93\u51fa"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"drivable_area: \u9a7e\u9a76\u9762\u79ef\u505a\u4e3a\u8f6e\u5ed3\u6570\u636e\u70b9"),(0,r.kt)("li",{parentName:"ul"},"lanes: lanes(\u672c\u53d8\u91cf)\u768460\u4e2a\u70b9\u503c\u8868\u793a\u8f66\u9053")),(0,r.kt)("h2",{id:"\u793a\u4f8b\u7ed8\u5236-\u8f66\u9053\u662f\u7ea2\u8272\u7684\u53ef\u9a7e\u9a76\u533a\u57df\u662f\u7eff\u8272\u7684"},"\u793a\u4f8b\u7ed8\u5236 (\u8f66\u9053\u662f\u7ea2\u8272\u7684\uff0c\u53ef\u9a7e\u9a76\u533a\u57df\u662f\u7eff\u8272\u7684)"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/I531NIT.gif",alt:"Imgur"})),(0,r.kt)("h2",{id:"\u56fe\u63cf\u8ff0"},"\u56fe\u63cf\u8ff0"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"  - id: yolop\n    operator: \n      outputs:\n        - lanes\n        - drivable_area\n      inputs:\n        image: webcam/image\n      python: ../../operators/yolop_op.py\n")),(0,r.kt)("h2",{id:"\u65b9\u6cd5"},"\u65b9\u6cd5"),(0,r.kt)("h3",{id:"__init__"},(0,r.kt)("inlineCode",{parentName:"h3"},"__init__()")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"\u6e90\u7801"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'    def __init__(self):\n        self.model = torch.hub.load("hustvl/yolop", "yolop", pretrained=True)\n        self.model.to(torch.device(DEVICE))\n        self.model.eval()\n\n\n'))),(0,r.kt)("h3",{id:"on_event"},(0,r.kt)("inlineCode",{parentName:"h3"},".on_event(...)")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"\u6e90\u7801"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\n    def on_event(\n        self,\n        dora_event: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        if dora_event["type"] == "INPUT":\n            return self.on_input(dora_event, send_output)\n        return DoraStatus.CONTINUE\n\n\n'))),(0,r.kt)("h3",{id:"on_input"},(0,r.kt)("inlineCode",{parentName:"h3"},".on_input(...)")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"\u6e90\u7801"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'\n    def on_input(\n        self,\n        dora_input: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        # \u63a8\u65ad\n        frame = cv2.imdecode(\n            np.frombuffer(\n                dora_input["data"],\n                np.uint8,\n            ),\n            -1,\n        )\n\n        frame = frame[:, :, :3]\n        h0, w0, _ = frame.shape\n        h, w = (640, 640)\n        frame, _, (pad_w, pad_h) = letterbox_for_img(frame)\n        ratio = w / w0\n        pad_h, pad_w = (int(pad_h), int(pad_w))\n\n        img = torch.unsqueeze(transform(frame), dim=0)\n        half = False  # half precision only supported on CUDA\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img = img.to(torch.device(DEVICE))\n        det_out, da_seg_out, ll_seg_out = self.model(img)\n\n        # det_out = [pred.reshape((1, -1, 6)) for pred in det_out]\n        # inf_out = torch.cat(det_out, dim=1)\n\n        # det_pred = non_max_suppression(\n        # inf_out,\n        # )\n        # det = det_pred[0]\n\n        da_predict = da_seg_out[:, :, pad_h : (h0 - pad_h), pad_w : (w0 - pad_w)]\n        da_seg_mask = torch.nn.functional.interpolate(\n            da_predict, scale_factor=1 / ratio, mode="bilinear"\n        )\n        _, da_seg_mask = torch.max(da_seg_mask, 1)\n        da_seg_mask = da_seg_mask.int().squeeze().cpu().numpy()\n        da_seg_mask = morphological_process(da_seg_mask, kernel_size=7)\n\n        contours, _ = cv2.findContours(\n            da_seg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n        if len(contours) != 0:\n            contour = max(contours, key=cv2.contourArea)\n            contour = contour.astype(np.int32)\n            send_output("drivable_area", contour.tobytes(), dora_input["metadata"])\n        else:\n            send_output("drivable_area", np.array([]).tobytes(), dora_input["metadata"])\n\n        ll_predict = ll_seg_out[:, :, pad_h : (h0 - pad_h), pad_w : (w0 - pad_w)]\n\n        ll_seg_mask = torch.nn.functional.interpolate(\n            ll_predict, scale_factor=1 / ratio, mode="bilinear"\n        )\n\n        _, ll_seg_mask = torch.max(ll_seg_mask, 1)\n        ll_seg_mask = ll_seg_mask.int().squeeze().cpu().numpy()\n        # Lane line post-processing\n        ll_seg_mask = morphological_process(\n            ll_seg_mask, kernel_size=7, func_type=cv2.MORPH_OPEN\n        )\n        ll_seg_points = np.array(connect_lane(ll_seg_mask), np.int32)\n        send_output("lanes", ll_seg_points.tobytes(), dora_input["metadata"])\n        return DoraStatus.CONTINUE\n\n\n'))))}_.isMDXComponent=!0}}]);