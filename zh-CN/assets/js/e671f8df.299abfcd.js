"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[4914],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>m});var r=t(7294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var p=r.createContext({}),s=function(e){var n=r.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=s(e.components);return r.createElement(p.Provider,{value:n},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},_=r.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=s(t),_=o,m=d["".concat(p,".").concat(_)]||d[_]||c[_]||a;return t?r.createElement(m,l(l({ref:n},u),{},{components:t})):r.createElement(m,l({ref:n},u))}));function m(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,l=new Array(a);l[0]=_;var i={};for(var p in n)hasOwnProperty.call(n,p)&&(i[p]=n[p]);i.originalType=e,i[d]="string"==typeof e?e:o,l[1]=i;for(var s=2;s<a;s++)l[s]=t[s];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}_.displayName="MDXCreateElement"},4706:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>c,frontMatter:()=>a,metadata:()=>i,toc:()=>s});var r=t(7462),o=(t(7294),t(3905));const a={},l="yolop operator",i={unversionedId:"nodes_operators/yolop_op",id:"nodes_operators/yolop_op",title:"yolop operator",description:"yolop recognizes lanes, and drivable area from a specific images.",source:"@site/i18n/zh-CN/docusaurus-plugin-content-docs/current/nodes_operators/yolop_op.md",sourceDirName:"nodes_operators",slug:"/nodes_operators/yolop_op",permalink:"/zh-CN/docs/nodes_operators/yolop_op",draft:!1,editUrl:"https://crowdin.com/dora-rs/zh-CN",tags:[],version:"current",frontMatter:{},sidebar:"nodes_operators",previous:{title:"Webcam operator",permalink:"/zh-CN/docs/nodes_operators/webcam_op"},next:{title:"Yolov5 operator",permalink:"/zh-CN/docs/nodes_operators/yolov5_op"}},p={},s=[{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Example plot ( lanes in red, drivable area in green)",id:"example-plot--lanes-in-red-drivable-area-in-green",level:2},{value:"Graph Description",id:"graph-description",level:2},{value:"Methods",id:"methods",level:2},{value:"<code>__init__()</code>",id:"__init__",level:3},{value:"<code>.on_event(...)</code>",id:"on_event",level:3},{value:"<code>.on_input(...)</code>",id:"on_input",level:3}],u={toc:s},d="wrapper";function c(e){let{components:n,...t}=e;return(0,o.kt)(d,(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"yolop-operator"},"yolop operator"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"yolop")," recognizes lanes, and drivable area from a specific images."),(0,o.kt)("p",null,"More info here: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hustvl/YOLOP"},"https://github.com/hustvl/YOLOP")),(0,o.kt)("p",null,"You can also choose to allocate the model in GPU using the environment variable:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"PYTORCH_DEVICE: cuda # or cpu"))),(0,o.kt)("h2",{id:"inputs"},"Inputs"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"image: HEIGHT x WIDTH x BGR array.")),(0,o.kt)("h2",{id:"outputs"},"Outputs"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"drivable_area: drivable area as contour points"),(0,o.kt)("li",{parentName:"ul"},"lanes: lanes as 60 points representing the lanes")),(0,o.kt)("h2",{id:"example-plot--lanes-in-red-drivable-area-in-green"},"Example plot ( lanes in red, drivable area in green)"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://i.imgur.com/I531NIT.gif",alt:"Imgur"})),(0,o.kt)("h2",{id:"graph-description"},"Graph Description"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"  - id: yolop\n    operator: \n      outputs:\n        - lanes\n        - drivable_area\n      inputs:\n        image: webcam/image\n      python: ../../operators/yolop_op.py\n")),(0,o.kt)("h2",{id:"methods"},"Methods"),(0,o.kt)("h3",{id:"__init__"},(0,o.kt)("inlineCode",{parentName:"h3"},"__init__()")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Source Code"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'    def __init__(self):\n        self.model = torch.hub.load("hustvl/yolop", "yolop", pretrained=True)\n        self.model.to(torch.device(DEVICE))\n        self.model.eval()\n\n\n'))),(0,o.kt)("h3",{id:"on_event"},(0,o.kt)("inlineCode",{parentName:"h3"},".on_event(...)")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Source Code"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\n    def on_event(\n        self,\n        dora_event: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        if dora_event["type"] == "INPUT":\n            return self.on_input(dora_event, send_output)\n        return DoraStatus.CONTINUE\n\n\n'))),(0,o.kt)("h3",{id:"on_input"},(0,o.kt)("inlineCode",{parentName:"h3"},".on_input(...)")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Source Code"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\n    def on_input(\n        self,\n        dora_input: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        # inference\n        frame = cv2.imdecode(\n            np.frombuffer(\n                dora_input["data"],\n                np.uint8,\n            ),\n            -1,\n        )\n\n        frame = frame[:, :, :3]\n        h0, w0, _ = frame.shape\n        h, w = (640, 640)\n        frame, _, (pad_w, pad_h) = letterbox_for_img(frame)\n        ratio = w / w0\n        pad_h, pad_w = (int(pad_h), int(pad_w))\n\n        img = torch.unsqueeze(transform(frame), dim=0)\n        half = False  # half precision only supported on CUDA\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img = img.to(torch.device(DEVICE))\n        det_out, da_seg_out, ll_seg_out = self.model(img)\n\n        # det_out = [pred.reshape((1, -1, 6)) for pred in det_out]\n        # inf_out = torch.cat(det_out, dim=1)\n\n        # det_pred = non_max_suppression(\n        # inf_out,\n        # )\n        # det = det_pred[0]\n\n        da_predict = da_seg_out[:, :, pad_h : (h0 - pad_h), pad_w : (w0 - pad_w)]\n        da_seg_mask = torch.nn.functional.interpolate(\n            da_predict, scale_factor=1 / ratio, mode="bilinear"\n        )\n        _, da_seg_mask = torch.max(da_seg_mask, 1)\n        da_seg_mask = da_seg_mask.int().squeeze().cpu().numpy()\n        da_seg_mask = morphological_process(da_seg_mask, kernel_size=7)\n\n        contours, _ = cv2.findContours(\n            da_seg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n        if len(contours) != 0:\n            contour = max(contours, key=cv2.contourArea)\n            contour = contour.astype(np.int32)\n            send_output("drivable_area", contour.tobytes(), dora_input["metadata"])\n        else:\n            send_output("drivable_area", np.array([]).tobytes(), dora_input["metadata"])\n\n        ll_predict = ll_seg_out[:, :, pad_h : (h0 - pad_h), pad_w : (w0 - pad_w)]\n\n        ll_seg_mask = torch.nn.functional.interpolate(\n            ll_predict, scale_factor=1 / ratio, mode="bilinear"\n        )\n\n        _, ll_seg_mask = torch.max(ll_seg_mask, 1)\n        ll_seg_mask = ll_seg_mask.int().squeeze().cpu().numpy()\n        # Lane line post-processing\n        ll_seg_mask = morphological_process(\n            ll_seg_mask, kernel_size=7, func_type=cv2.MORPH_OPEN\n        )\n        ll_seg_points = np.array(connect_lane(ll_seg_mask), np.int32)\n        send_output("lanes", ll_seg_points.tobytes(), dora_input["metadata"])\n        return DoraStatus.CONTINUE\n\n\n'))))}c.isMDXComponent=!0}}]);