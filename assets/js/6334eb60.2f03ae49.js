"use strict";(self.webpackChunkdora_rs_github_io=self.webpackChunkdora_rs_github_io||[]).push([[8744],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>_});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=d(n),c=o,_=u["".concat(s,".").concat(c)]||u[c]||m[c]||r;return n?a.createElement(_,i(i({ref:t},p),{},{components:n})):a.createElement(_,i({ref:t},p))}));function _(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:o,i[1]=l;for(var d=2;d<r;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},5036:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var a=n(7462),o=(n(7294),n(3905));const r={},i="MiDaS",l={unversionedId:"nodes_operators/midas_op",id:"nodes_operators/midas_op",title:"MiDaS",description:"MiDaS models for computing relative depth from a single image.",source:"@site/docs/nodes_operators/midas_op.md",sourceDirName:"nodes_operators",slug:"/nodes_operators/midas_op",permalink:"/docs/nodes_operators/midas_op",draft:!1,editUrl:"https://github.com/dora-rs/dora-rs.github.io/edit/main/docs/nodes_operators/midas_op.md",tags:[],version:"current",frontMatter:{},sidebar:"nodes_operators",previous:{title:"FOT operator",permalink:"/docs/nodes_operators/fot_op"},next:{title:"Obstacle location operator",permalink:"/docs/nodes_operators/obstacle_location_op"}},s={},d=[{value:"Installation:",id:"installation",level:3},{value:"Methods",id:"methods",level:2},{value:"<code>__init__()</code>",id:"__init__",level:3},{value:"<code>.on_event(...)</code>",id:"on_event",level:3},{value:"<code>.on_input(...)</code>",id:"on_input",level:3}],p={toc:d},u="wrapper";function m(e){let{components:t,...n}=e;return(0,o.kt)(u,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"midas"},"MiDaS"),(0,o.kt)("p",null,"MiDaS models for computing relative depth from a single image."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs.")),(0,o.kt)("h3",{id:"installation"},"Installation:"),(0,o.kt)("p",null,"To install midas offline:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cd $DORA_DEP_HOME/dependencies/\ngit clone git@github.com:isl-org/MiDaS.git\ncd MiDaS/weights\n# If you don't want to add manual download, the program will also automatically download the model file\nwget https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt\ncp midas_v21_small_256.pt $HOME/.cache/torch/hub/checkpoints/\n")),(0,o.kt)("p",null,"Add the following dataflow configuration "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'  - id: midas_op\n    operator:\n      outputs:\n        - depth_frame\n      inputs:\n        image: webcam/image\n      python: ../../operators/midas_op.py\n    env:\n      PYTORCH_DEVICE: "cuda"\n      MIDAS_PATH: $DORA_DEP_HOME/dependencies/MiDaS/\n      MIDAS_WEIGHT_PATH: $DORA_DEP_HOME/dependencies/MiDaS/weights/midas_v21_small_256.pt\n      MODEL_TYPE: "MiDaS_small"\n      MODEL_NAME: "MiDaS_small"\n')),(0,o.kt)("blockquote",null,(0,o.kt)("ul",{parentName:"blockquote"},(0,o.kt)("li",{parentName:"ul"},'model_type = "DPT_Large"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)'),(0,o.kt)("li",{parentName:"ul"},'model_type = "DPT_Hybrid"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)'),(0,o.kt)("li",{parentName:"ul"},'model_type = "MiDaS_small"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)'))),(0,o.kt)("h2",{id:"methods"},"Methods"),(0,o.kt)("h3",{id:"__init__"},(0,o.kt)("inlineCode",{parentName:"h3"},"__init__()")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Source Code"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'    def __init__(self):\n        if MIDAS_PATH is None:\n            # With internet\n            self.model = torch.hub.load(\n                "intel-isl/MiDaS",\n                MODEL_TYPE,\n            )\n            midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")\n        else:\n            # Without internet\n            self.model = torch.hub.load(\n                repo_or_dir=MIDAS_PATH,\n                model=MODEL_NAME,\n                weights=MIDAS_WEIGHT_PATH,\n                source="local",\n            )\n            midas_transforms = torch.hub.load(\n                repo_or_dir=MIDAS_PATH, model="transforms", source="local"\n            )\n        if MODEL_TYPE == "DPT_Large" or MODEL_TYPE == "DPT_Hybrid":\n            self.transform = midas_transforms.dpt_transform\n        else:\n            self.transform = midas_transforms.small_transform\n        self.model.to(torch.device(DEVICE))\n        self.model.eval()\n\n\n'))),(0,o.kt)("h3",{id:"on_event"},(0,o.kt)("inlineCode",{parentName:"h3"},".on_event(...)")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Source Code"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\n    def on_event(\n        self,\n        dora_event: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        if dora_event["type"] == "INPUT":\n            return self.on_input(dora_event, send_output)\n        return DoraStatus.CONTINUE\n\n\n'))),(0,o.kt)("h3",{id:"on_input"},(0,o.kt)("inlineCode",{parentName:"h3"},".on_input(...)")),(0,o.kt)("p",null,"Handle image\nArgs:\ndora_input",'["id"]',"  (str): Id of the input declared in the yaml configuration\ndora_input",'["data"]'," (bytes): Bytes message of the input\nsend_output (Callable[","[str, bytes]","]): Function enabling sending output back to dora."),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Source Code"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\n    def on_input(\n        self,\n        dora_input: dict,\n        send_output: Callable[[str, bytes], None],\n    ) -> DoraStatus:\n        """Handle image\n        Args:\n            dora_input["id"]  (str): Id of the input declared in the yaml configuration\n            dora_input["data"] (bytes): Bytes message of the input\n            send_output (Callable[[str, bytes]]): Function enabling sending output back to dora.\n        """\n        if dora_input["id"] == "image":\n            # Convert bytes to numpy array\n            frame = np.frombuffer(\n                dora_input["data"],\n                np.uint8,\n            ).reshape((IMAGE_HEIGHT, IMAGE_WIDTH, 4))\n\n            with torch.no_grad():\n                image = frame[:, :, :3]\n                img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                input_batch = self.transform(img).to(DEVICE)\n                prediction = self.model(input_batch)\n                prediction = torch.nn.functional.interpolate(\n                    prediction.unsqueeze(1),\n                    size=img.shape[:2],\n                    mode="bicubic",\n                    align_corners=False,\n                ).squeeze()\n                depth_output = prediction.cpu().numpy()\n                depth_min = depth_output.min()\n                depth_max = depth_output.max()\n                normalized_depth = (\n                    255 * (depth_output - depth_min) / (depth_max - depth_min)\n                )\n                normalized_depth *= 3\n                depth_frame = (\n                    np.repeat(np.expand_dims(normalized_depth, 2), 3, axis=2)\n                    / 3\n                )\n                depth_frame = cv2.applyColorMap(\n                    np.uint8(depth_frame), cv2.COLORMAP_INFERNO\n                )\n                height, width = depth_frame.shape[:2]\n                depth_frame_4 = np.dstack(\n                    [depth_frame, np.ones((height, width), dtype="uint8") * 255]\n                )\n\n                send_output(\n                    "depth_frame",\n                    depth_frame_4.tobytes(),\n                    dora_input["metadata"],\n                )\n        return DoraStatus.CONTINUE\n\n\n'))))}m.isMDXComponent=!0}}]);